{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancer Detection using InceptionNet V3\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in the datasets\n",
    "----\n",
    "Info on how to obtain the datasets is contained in the README.md. Data augmentation techniques are applied in the data loader here to be applied to the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write data loaders for training, validation, and test sets\n",
    "## Specify appropriate transforms, and batch_sizes\n",
    "train_transfoms = transforms.Compose([transforms.Resize((299 , 299))\n",
    "                              ,transforms.RandomRotation(degrees = 10)\n",
    "                              ,transforms.RandomHorizontalFlip(p=0.2)\n",
    "                              ,transforms.RandomGrayscale(p=0.2)\n",
    "                              ,transforms.RandomVerticalFlip(p=0.2)\n",
    "                              ,transforms.ToTensor()\n",
    "                              ,transforms.Normalize(mean=[0.485, 0.456, 0.406]\n",
    "                                                   , std=[0.229, 0.224, 0.225])\n",
    "                              ])\n",
    "\n",
    "test_val_transfomrs = transforms.Compose([transforms.Resize((299 , 299))\n",
    "                              ,transforms.ToTensor()\n",
    "                              ,transforms.Normalize(mean=[0.485, 0.456, 0.406]\n",
    "                                                   , std = [0.229, 0.224, 0.225])\n",
    "                              ])\n",
    "\n",
    "batch_size = 4 #About all my little GPU can handle \n",
    "num_workers = 0\n",
    "\n",
    "train_data = datasets.ImageFolder(root=r'C:\\Users\\diarm\\Downloads\\cancer_detection\\data\\train'\n",
    "                                  , transform=train_transfoms)\n",
    "test_data = datasets.ImageFolder(root=r'C:\\Users\\diarm\\Downloads\\cancer_detection\\data\\test'\n",
    "                                 , transform=test_val_transfomrs)\n",
    "valid_data = datasets.ImageFolder(root=r'C:\\Users\\diarm\\Downloads\\cancer_detection\\data\\valid'\n",
    "                                  , transform=test_val_transfomrs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "\n",
    "loaders = dict(train=train_loader, test=test_loader, valid=valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are three class of skin lesion to look at in the problem.\n",
    "    - melanoma\n",
    "    - nevus\n",
    "    - seborrheic_keratosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "melanoma\n",
      "nevus\n",
      "seborrheic_keratosis\n"
     ]
    }
   ],
   "source": [
    "!ls C:\\\\Users\\\\diarm\\\\Downloads\\\\cancer_detection\\\\data\\\\train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_net = models.inception_v3(pretrained=True)\n",
    "\n",
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# move model to GPU if CUDA is available\n",
    "if use_cuda:\n",
    "    inception_net = inception_net.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The final classifier can be viewed here. \n",
    "For transfer learning we will change this final layer to suit our case and retrain it with our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "----\n",
    "A new fully connected layer is appended to the inception net structure. We are replacing the current fully connected layer which takes in 2048 features and has 1000 outputs. \n",
    "The InceptionNet model has been trained on the ImageNet dataset which is a very large collection of images curated into 1 of 1000 classes.\n",
    "\n",
    "I'm going to use the underlying layers of the network to piggyback on the extraction of features from an image, and just train the final layer to classify the images from our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the parameters of the network\n",
    "#for param in inception_net.parameters():\n",
    "#    param.requires_grad = False\n",
    "\n",
    "# Two final fully connected layer for output\n",
    "classifier = nn.Sequential(nn.Linear(2048, 512)\n",
    "                           ,nn.ReLU()\n",
    "                           #,nn.Dropout(0.1)\n",
    "                           ,nn.Linear(512, 64)\n",
    "                           ,nn.ReLU()\n",
    "                           #,nn.Dropout(0.1)\n",
    "                           ,nn.Linear(64, 3)\n",
    "                          )\n",
    "inception_net.fc = classifier\n",
    "\n",
    "# Replace layer on aux outputs\n",
    "aux_classifier = nn.Sequential(nn.Linear(768, 64)\n",
    "                               ,nn.ReLU()\n",
    "                               #,nn.Dropout(0.1)\n",
    "                               ,nn.Linear(64, 3)\n",
    "                              )\n",
    "inception_net.AuxLogits.fc = aux_classifier \n",
    "\n",
    "# Transfer to GPU if available\n",
    "if use_cuda:\n",
    "    inception_net = inception_net.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Sequential(\n",
       "   (0): Linear(in_features=768, out_features=64, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=64, out_features=3, bias=True)\n",
       " ),\n",
       " Sequential(\n",
       "   (0): Linear(in_features=2048, out_features=512, bias=True)\n",
       "   (1): ReLU()\n",
       "   (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "   (3): ReLU()\n",
       "   (4): Linear(in_features=64, out_features=3, bias=True)\n",
       " ))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inception_net.AuxLogits.fc, inception_net.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params_to_update = inception_net.parameters()\n",
    "#print(\"Params to learn:\")\n",
    "\n",
    "#params_to_update = []\n",
    "#for name,param in inception_net.named_parameters():\n",
    "#    if param.requires_grad == True:\n",
    "#        params_to_update.append(param)\n",
    "#        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# These values were obtained from here https://arxiv.org/ftp/arxiv/papers/1810/1810.10348.pdf\n",
    "optimizer = optim.SGD(inception_net.parameters(), lr=0.001)\n",
    "#scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function to show classification accuracy\n",
    "def label_accuracy(model, validation_loader, epoch, use_cuda):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, targets in validation_loader:\n",
    "            if use_cuda:\n",
    "                images, targets = images.cuda(), targets.cuda()\n",
    "            outputs = model(images)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (preds == targets).sum().item()\n",
    "    print('Accuray test at epoch {}\\tLabel Accuracy : - {} %'.format(epoch, round(correct / total, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "----\n",
    "\n",
    "**N.B Loss calculation is different with InceptionNet**\n",
    "\n",
    "It produces two outputs, the standard output and the auxilliary output. The auxiliary outputs is used to prevent the vanishing gradient problem that can occur in very deep networks. \n",
    "Loss is calculated like so: \n",
    "```python\n",
    "outputs, aux_outputs = model(data)\n",
    "loss1 = criterion(outputs, target)\n",
    "loss2 = criterion(aux_outputs, target)\n",
    "loss = loss1 + 0.4*loss2\n",
    "```\n",
    "\n",
    "This is described in the pytorch documentation [here](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the trainig function\n",
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # clear the accumulated gradients \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # inception net produces two outputs, aux output handles vanishing gradient\n",
    "            outputs, aux_outputs = model(data)\n",
    "            #loss calculation https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "            loss1 = criterion(outputs, target)\n",
    "            loss2 = criterion(aux_outputs, target)\n",
    "            loss = loss1 + 0.4*loss2 # 0.4 is weight for auxillary classifier\n",
    "            \n",
    "            # gradient of the loss with respect to the parameters\n",
    "            loss.backward()\n",
    "            # perform the parameter update (update the weights)\n",
    "            optimizer.step()\n",
    "            \n",
    "            ## record the average training loss\n",
    "            # train loss - adds current loss to accumulated loss (averaged over batch size)\n",
    "            train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "        \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            # get prediction\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            ## update the average validation loss\n",
    "            valid_loss = valid_loss + ((1 / (batch_idx + 1)) * (loss.data - valid_loss))\n",
    "            \n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "        \n",
    "        ## save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))\n",
    "            torch.save(model.state_dict(), save_path) #add save path here\n",
    "            valid_loss_min = valid_loss\n",
    "            # get epoch accuracy \n",
    "            #label_accuracy(model, loaders['valid'], epoch, use_cuda)\n",
    "        \n",
    "        # learning rate decay scheduled here if necessary\n",
    "        #scheduler.step()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.269913 \tValidation Loss: 1.026312\n",
      "Validation loss decreased (inf --> 1.026312).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.110748 \tValidation Loss: 0.978310\n",
      "Validation loss decreased (1.026312 --> 0.978310).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.071847 \tValidation Loss: 0.930630\n",
      "Validation loss decreased (0.978310 --> 0.930630).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.048700 \tValidation Loss: 0.903939\n",
      "Validation loss decreased (0.930630 --> 0.903939).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.006908 \tValidation Loss: 0.894121\n",
      "Validation loss decreased (0.903939 --> 0.894121).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.000296 \tValidation Loss: 0.868027\n",
      "Validation loss decreased (0.894121 --> 0.868027).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.982717 \tValidation Loss: 0.859777\n",
      "Validation loss decreased (0.868027 --> 0.859777).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.977019 \tValidation Loss: 0.821363\n",
      "Validation loss decreased (0.859777 --> 0.821363).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.951068 \tValidation Loss: 0.809533\n",
      "Validation loss decreased (0.821363 --> 0.809533).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.924613 \tValidation Loss: 0.811938\n",
      "Epoch: 11 \tTraining Loss: 0.916994 \tValidation Loss: 0.808546\n",
      "Validation loss decreased (0.809533 --> 0.808546).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.920027 \tValidation Loss: 0.794979\n",
      "Validation loss decreased (0.808546 --> 0.794979).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.854461 \tValidation Loss: 0.775582\n",
      "Validation loss decreased (0.794979 --> 0.775582).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.866777 \tValidation Loss: 0.795428\n",
      "Epoch: 15 \tTraining Loss: 0.836031 \tValidation Loss: 0.776373\n",
      "Epoch: 16 \tTraining Loss: 0.839436 \tValidation Loss: 0.781059\n",
      "Epoch: 17 \tTraining Loss: 0.828776 \tValidation Loss: 0.757506\n",
      "Validation loss decreased (0.775582 --> 0.757506).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.815926 \tValidation Loss: 0.771695\n",
      "Epoch: 19 \tTraining Loss: 0.810733 \tValidation Loss: 0.752574\n",
      "Validation loss decreased (0.757506 --> 0.752574).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.786252 \tValidation Loss: 0.726865\n",
      "Validation loss decreased (0.752574 --> 0.726865).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.764986 \tValidation Loss: 0.731025\n",
      "Epoch: 22 \tTraining Loss: 0.757956 \tValidation Loss: 0.704730\n",
      "Validation loss decreased (0.726865 --> 0.704730).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.738553 \tValidation Loss: 0.769689\n",
      "Epoch: 24 \tTraining Loss: 0.714634 \tValidation Loss: 0.744114\n",
      "Epoch: 25 \tTraining Loss: 0.668940 \tValidation Loss: 0.710968\n",
      "Epoch: 26 \tTraining Loss: 0.685258 \tValidation Loss: 0.669577\n",
      "Validation loss decreased (0.704730 --> 0.669577).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.648044 \tValidation Loss: 0.621221\n",
      "Validation loss decreased (0.669577 --> 0.621221).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.664165 \tValidation Loss: 0.646043\n",
      "Epoch: 29 \tTraining Loss: 0.592084 \tValidation Loss: 0.646120\n",
      "Epoch: 30 \tTraining Loss: 0.602272 \tValidation Loss: 0.682378\n",
      "Epoch: 31 \tTraining Loss: 0.570610 \tValidation Loss: 0.594539\n",
      "Validation loss decreased (0.621221 --> 0.594539).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.550042 \tValidation Loss: 0.560135\n",
      "Validation loss decreased (0.594539 --> 0.560135).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.514539 \tValidation Loss: 0.582660\n",
      "Epoch: 34 \tTraining Loss: 0.522774 \tValidation Loss: 0.624094\n",
      "Epoch: 35 \tTraining Loss: 0.478584 \tValidation Loss: 0.660270\n",
      "Epoch: 36 \tTraining Loss: 0.480741 \tValidation Loss: 0.626694\n",
      "Epoch: 37 \tTraining Loss: 0.438073 \tValidation Loss: 0.563711\n",
      "Epoch: 38 \tTraining Loss: 0.433206 \tValidation Loss: 0.598095\n",
      "Epoch: 39 \tTraining Loss: 0.411924 \tValidation Loss: 0.518835\n",
      "Validation loss decreased (0.560135 --> 0.518835).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.440803 \tValidation Loss: 0.606206\n",
      "Epoch: 41 \tTraining Loss: 0.410349 \tValidation Loss: 0.595896\n",
      "Epoch: 42 \tTraining Loss: 0.376219 \tValidation Loss: 0.585743\n",
      "Epoch: 43 \tTraining Loss: 0.381671 \tValidation Loss: 0.696414\n",
      "Epoch: 44 \tTraining Loss: 0.414903 \tValidation Loss: 0.683163\n",
      "Epoch: 45 \tTraining Loss: 0.359961 \tValidation Loss: 0.627442\n",
      "Epoch: 46 \tTraining Loss: 0.334596 \tValidation Loss: 0.668893\n",
      "Epoch: 47 \tTraining Loss: 0.335613 \tValidation Loss: 0.697263\n",
      "Epoch: 48 \tTraining Loss: 0.337531 \tValidation Loss: 0.667446\n",
      "Epoch: 49 \tTraining Loss: 0.292883 \tValidation Loss: 0.742613\n",
      "Epoch: 50 \tTraining Loss: 0.292292 \tValidation Loss: 0.713261\n"
     ]
    }
   ],
   "source": [
    "# let 'er riiiip\n",
    "trained_model = train(50, loaders, inception_net, optimizer, criterion, use_cuda, save_path='./model_temp/model_cancer_detection.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray test at epoch 50\tLabel Accuracy : - 0.75 %\n"
     ]
    }
   ],
   "source": [
    "epoch = 50\n",
    "label_accuracy(trained_model, loaders['valid'], epoch, use_cuda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
